{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Modified Taxi Problem**\n",
        "    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\""
      ],
      "metadata": {
        "id": "UwvMHGX-zMqE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "WHIf-5N2JuXg",
        "outputId": "9dbe4bb9-c814-4cb0-e234-959ddff9a703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (71.0.4)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-72.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Using cached setuptools-72.1.0-py3-none-any.whl (2.3 MB)\n",
            "Installing collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 71.0.4\n",
            "    Uninstalling setuptools-71.0.4:\n",
            "      Successfully uninstalled setuptools-71.0.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-72.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              },
              "id": "c56d3a00a73e406e8a418bed850a76b8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install --upgrade gym==0..0\n",
        "!pip install gym.envs.toy_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from gym.envs.toy_text import discrete\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import math\n",
        "from matplotlib import rc\n",
        "import sys\n",
        "from contextlib import closing\n",
        "from io import StringIO\n",
        "from gym import utils\n",
        "from gym.envs.registration import register\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.3)\n",
        "\n",
        "rcParams['figure.figsize'] = 14, 8\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "5UYJXpsVKisB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAP = [\n",
        "    \"+---------+\",\n",
        "    \"|R: :A: :G|\",\n",
        "    \"| : : : : |\",\n",
        "    \"| :A:A: : |\",\n",
        "    \"| : : : : |\",\n",
        "    \"|Y: :A:B: |\",\n",
        "    \"+---------+\",\n",
        "]\n",
        "\n",
        "\n",
        "class SafeCab(discrete.DiscreteEnv):\n",
        "    \"\"\"\n",
        "    Modified Taxi Problem\n",
        "    from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
        "    by Tom Dietterich\n",
        "\n",
        "    Description:\n",
        "    There are four designated locations in the grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drive to the passenger's location, pick up the passenger, drive to the passenger's destination (another one of the four specified locations), and then drop off the passenger. The episode ends if the taxi ends up on anomaly location. Once the passenger is dropped off, the episode ends.\n",
        "\n",
        "    Observations:\n",
        "    There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is the taxi), and 4 destination locations.\n",
        "\n",
        "    Actions:\n",
        "    There are 6 discrete deterministic actions:\n",
        "    - 0: move south\n",
        "    - 1: move north\n",
        "    - 2: move east\n",
        "    - 3: move west\n",
        "    - 4: pickup passenger\n",
        "    - 5: dropoff passenger\n",
        "\n",
        "    Rewards:\n",
        "    There is a reward of -1 for each action and an additional reward of +1000 for delievering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally. Interaction with an anomaly gives -1000 reward and end the episode.\n",
        "\n",
        "\n",
        "    Rendering:\n",
        "    - blue: passenger\n",
        "    - magenta: destination\n",
        "    - yellow: empty taxi\n",
        "    - green: full taxi\n",
        "    - red: anomaly\n",
        "    - other letters (R, G, B and Y): locations for passengers and destinations\n",
        "\n",
        "    actions:\n",
        "    - 0: south\n",
        "    - 1: north\n",
        "    - 2: east\n",
        "    - 3: west\n",
        "    - 4: pickup\n",
        "    - 5: dropoff\n",
        "\n",
        "    state space is represented by:\n",
        "        (taxi_row, taxi_col, passenger_location, destination)\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['human', 'ansi']}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.desc = np.asarray(MAP, dtype='c')\n",
        "\n",
        "        self.locs = locs = [(0,0), (0,4), (4,0), (4,3)]\n",
        "        self.anomaly_locs = [(0,2), (2,1), (2,2), (4,2)]\n",
        "\n",
        "        num_states = 500\n",
        "        num_rows = 5\n",
        "        num_columns = 5\n",
        "        max_row = num_rows - 1\n",
        "        max_col = num_columns - 1\n",
        "        initial_state_distrib = np.zeros(num_states)\n",
        "        num_actions = 6\n",
        "        P = {state: {action: []\n",
        "                     for action in range(num_actions)} for state in range(num_states)}\n",
        "        for row in range(num_rows):\n",
        "            for col in range(num_columns):\n",
        "                for pass_idx in range(len(locs) + 1):  # +1 for being inside taxi\n",
        "                    for dest_idx in range(len(locs)):\n",
        "                        state = self.encode(row, col, pass_idx, dest_idx)\n",
        "                        if pass_idx < 4 and pass_idx != dest_idx:\n",
        "                            initial_state_distrib[state] += 1\n",
        "                        for action in range(num_actions):\n",
        "                            # defaults\n",
        "                            new_row, new_col, new_pass_idx = row, col, pass_idx\n",
        "                            reward = -1 # default reward when there is no pickup/dropoff\n",
        "                            done = False\n",
        "                            taxi_loc = (row, col)\n",
        "\n",
        "                            if action == 0:\n",
        "                                new_row = min(row + 1, max_row)\n",
        "                            elif action == 1:\n",
        "                                new_row = max(row - 1, 0)\n",
        "                            if action == 2:\n",
        "                                new_col = min(col + 1, max_col)\n",
        "                            elif action == 3:\n",
        "                                new_col = max(col - 1, 0)\n",
        "                            elif action == 4:  # pickup\n",
        "                                if (pass_idx < 4 and taxi_loc == locs[pass_idx]):\n",
        "                                    new_pass_idx = 4\n",
        "                                else: # passenger not at location\n",
        "                                    reward = -10\n",
        "                            elif action == 5:  # dropoff\n",
        "                                if (taxi_loc == locs[dest_idx]) and pass_idx == 4:\n",
        "                                    new_pass_idx = dest_idx\n",
        "                                    done = True\n",
        "                                    reward = 1000\n",
        "                                elif (taxi_loc in locs) and pass_idx == 4:\n",
        "                                    new_pass_idx = locs.index(taxi_loc)\n",
        "                                else: # dropoff at wrong location\n",
        "                                    reward = -10\n",
        "\n",
        "                            new_loc = (new_row, new_col)\n",
        "                            if new_loc in self.anomaly_locs:\n",
        "                              reward = -1000\n",
        "                              done = True\n",
        "                            new_state = self.encode(\n",
        "                                new_row, new_col, new_pass_idx, dest_idx)\n",
        "                            P[state][action].append(\n",
        "                                (1.0, new_state, reward, done))\n",
        "        initial_state_distrib /= initial_state_distrib.sum()\n",
        "        discrete.DiscreteEnv.__init__(\n",
        "            self, num_states, num_actions, P, initial_state_distrib)\n",
        "\n",
        "    def encode(self, taxi_row, taxi_col, pass_loc, dest_idx):\n",
        "        # (5) 5, 5, 4\n",
        "        i = taxi_row\n",
        "        i *= 5\n",
        "        i += taxi_col\n",
        "        i *= 5\n",
        "        i += pass_loc\n",
        "        i *= 4\n",
        "        i += dest_idx\n",
        "        return i\n",
        "\n",
        "    def decode(self, i):\n",
        "        out = []\n",
        "        out.append(i % 4)\n",
        "        i = i // 4\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i % 5)\n",
        "        i = i // 5\n",
        "        out.append(i)\n",
        "        assert 0 <= i < 5\n",
        "        return reversed(out)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
        "\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        taxi_row, taxi_col, pass_idx, dest_idx = self.decode(self.s)\n",
        "\n",
        "        def ul(x): return \"_\" if x == \" \" else x\n",
        "\n",
        "        if pass_idx < 4:\n",
        "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
        "                out[1 + taxi_row][2 * taxi_col + 1], 'yellow', highlight=True)\n",
        "            pi, pj = self.locs[pass_idx]\n",
        "            out[1 + pi][2 * pj + 1] = utils.colorize(out[1 + pi][2 * pj + 1], 'blue', bold=True)\n",
        "        else:  # passenger in taxi\n",
        "            out[1 + taxi_row][2 * taxi_col + 1] = utils.colorize(\n",
        "                ul(out[1 + taxi_row][2 * taxi_col + 1]), 'green', highlight=True)\n",
        "\n",
        "        di, dj = self.locs[dest_idx]\n",
        "        out[1 + di][2 * dj + 1] = utils.colorize(out[1 + di][2 * dj + 1], 'magenta')\n",
        "\n",
        "        for (zx, zy) in self.anomaly_locs:\n",
        "          out[1 + zx][2 * zy + 1] = utils.colorize(\n",
        "            out[1 + zx][2 * zy + 1], 'red', bold=True)\n",
        "\n",
        "        outfile.write(\"\\n\".join([\"\".join(row) for row in out]) + \"\\n\")\n",
        "        if self.lastaction is not None:\n",
        "            outfile.write(\"  ({})\\n\".format([\"South\", \"North\", \"East\", \"West\", \"Pickup\", \"Dropoff\"][self.lastaction]))\n",
        "        else: outfile.write(\"\\n\")\n",
        "\n",
        "        # No need to return anything for human\n",
        "        if mode != 'human':\n",
        "            with closing(outfile):\n",
        "                return outfile.getvalue()\n",
        "\n",
        "register(\n",
        "    id='SafeCab-v0',\n",
        "    entry_point=f\"{__name__}:SafeCab\",\n",
        "    timestep_limit=100,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "sNR4dYKbTXsI",
        "outputId": "b1aea009-e757-4229-cd13-f5e83b42d24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-eb947b5ef6e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m register(\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SafeCab-v0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mentry_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{__name__}:SafeCab\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, id, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Overriding environment {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'timestep_limit'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_frames(frames):\n",
        "  for i, frame in enumerate(frames):\n",
        "    clear_output(wait=True)\n",
        "    print(frame['frame'])\n",
        "    print(f\"Episode: {frame['episode']}\")\n",
        "    print(f\"Timestep: {frame['step']}\")\n",
        "    print(f\"State: {frame['state']}\")\n",
        "    print(f\"Reward: {frame['reward']}\")\n",
        "    time.sleep(.4)"
      ],
      "metadata": {
        "id": "wUck5O5YTcmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('SafeCab-v0')"
      ],
      "metadata": {
        "id": "n9HkaqkpTgBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "metadata": {
        "id": "IJIWotpgTivZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "id": "QovbgdbcTjtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_size = env.action_space.n\n",
        "print(\"Action size \", action_size)\n",
        "\n",
        "state_size = env.observation_space.n\n",
        "print(\"State size \", state_size)"
      ],
      "metadata": {
        "id": "L2Wk1Z1dTnqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "\n",
        "  def __init__(self, n_states, n_actions, decay_rate=0.0001, learning_rate=0.7, gamma=0.618):\n",
        "    self.n_actions = n_actions\n",
        "    self.q_table = np.zeros((n_states, n_actions))\n",
        "    self.epsilon = 1.0\n",
        "    self.max_epsilon = 1.0\n",
        "    self.min_epsilon = 0.01\n",
        "    self.decay_rate = decay_rate\n",
        "    self.learning_rate = learning_rate\n",
        "    self.gamma = gamma # discount rate\n",
        "    self.epsilons_ = []\n",
        "\n",
        "  def choose_action(self, explore=True):\n",
        "    exploration_tradeoff = np.random.uniform(0, 1)\n",
        "\n",
        "    if explore and exploration_tradeoff < self.epsilon:\n",
        "      # exploration\n",
        "      return np.random.randint(self.n_actions)\n",
        "    else:\n",
        "      # exploitation (taking the biggest Q value for this state)\n",
        "      return np.argmax(self.q_table[state, :])\n",
        "\n",
        "  def learn(self, state, action, reward, next_state, done, episode):\n",
        "    # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "    self.q_table[state, action] = self.q_table[state, action] + \\\n",
        "      self.learning_rate * (reward + self.gamma * \\\n",
        "        np.max(self.q_table[new_state, :]) - self.q_table[state, action])\n",
        "\n",
        "    if done:\n",
        "      # Reduce epsilon to decrease the exploration over time\n",
        "      self.epsilon = self.min_epsilon + (self.max_epsilon - self.min_epsilon) * \\\n",
        "        np.exp(-self.decay_rate * episode)\n",
        "      self.epsilons_.append(self.epsilon)"
      ],
      "metadata": {
        "id": "uv85myepTs5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_episodes = 60000\n",
        "total_test_episodes = 10\n",
        "\n",
        "agent = Agent(env.observation_space.n, env.action_space.n)"
      ],
      "metadata": {
        "id": "EV7uxY7rTt0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "untrained_frames = []\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "    state = env.reset()\n",
        "\n",
        "    step = 1\n",
        "\n",
        "    while True:\n",
        "        action = agent.choose_action()\n",
        "\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        untrained_frames.append({\n",
        "          'frame': env.render(mode='ansi'),\n",
        "          'state': state,\n",
        "          'episode': episode + 1,\n",
        "          'step': step,\n",
        "          'reward': reward\n",
        "        })\n",
        "\n",
        "        if done:\n",
        "            step = 0\n",
        "            break\n",
        "        state = new_state\n",
        "        step += 1"
      ],
      "metadata": {
        "id": "P_UceKgMTx77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_frames(untrained_frames)"
      ],
      "metadata": {
        "id": "xaLRqzD5Tyzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = []\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "    state = env.reset()\n",
        "    episode_rewards = []\n",
        "\n",
        "    while True:\n",
        "\n",
        "        action = agent.choose_action()\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        agent.learn(state, action, reward, new_state, done, episode)\n",
        "\n",
        "        state = new_state\n",
        "\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        if done == True:\n",
        "          break\n",
        "\n",
        "    rewards.append(np.mean(episode_rewards))"
      ],
      "metadata": {
        "id": "l4-vcmLiT3Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(savgol_filter(rewards, 1001, 2))\n",
        "plt.title(\"Smoothened training reward per episode\")\n",
        "plt.xlabel('Episode');\n",
        "plt.ylabel('Total Reward');"
      ],
      "metadata": {
        "id": "-CGfXqZ_T3yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(agent.epsilons_)\n",
        "plt.title(\"Epsilon for episode\")\n",
        "plt.xlabel('Episode');\n",
        "plt.ylabel('Epsilon');"
      ],
      "metadata": {
        "id": "mt5TYPyeT7yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = []\n",
        "\n",
        "rewards = []\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "    state = env.reset()\n",
        "    episode_rewards = []\n",
        "\n",
        "    step = 1\n",
        "\n",
        "    while True:\n",
        "        action = agent.choose_action(explore=False)\n",
        "\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        frames.append({\n",
        "          'frame': env.render(mode='ansi'),\n",
        "          'state': state,\n",
        "          'episode': episode + 1,\n",
        "          'step': step,\n",
        "          'reward': reward\n",
        "        })\n",
        "\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        if done:\n",
        "            step = 0\n",
        "            break\n",
        "        state = new_state\n",
        "        step += 1\n",
        "\n",
        "    rewards.append(np.mean(episode_rewards))\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "zddLMYJVT-ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rewards)\n",
        "plt.title(\"Test reward per episode\")\n",
        "plt.ylim((0, 150))\n",
        "plt.xlabel('Episode');\n",
        "plt.ylabel('Total Reward');"
      ],
      "metadata": {
        "id": "MsHiK_VTT_be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_frames(frames)"
      ],
      "metadata": {
        "id": "q_kOsDeoUBqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}